{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storm Detection and Tracking\n",
    "## Code Written by: Eric Oliver\n",
    "### Edited by Robert Fritzen for NIU WCS Storm Tracking Project\n",
    "\n",
    "**Python Script Used for:** Fritzen, R., V. Lang, and V. Gensini, 2020: Trends and Variability of North American Extratropical Cyclones: 1979â€“2019. *J. Appl. Met. Cli.*, Submitted\n",
    "\n",
    "The original script (https://github.com/ecjoliver/stormTracking) was written to detect cyclones and anticyclones for the twentieth century reanalysis (20CR) dataset, although it employed Python 2 and a few packages that have since lost support or feature now depricated methods.\n",
    "\n",
    "This notebook features the edited scripts, along with Python 3 compliance and the introduction of replacement packages that are more \"properly\" maintained. This notebook can be run on a clean anaconda-3 installation with only a few extra packages required.\n",
    "\n",
    "Required Packages:\n",
    "* numpy\n",
    "* scipy\n",
    "* matplotlib\n",
    "* netCDF4\n",
    "\n",
    "This script has been tested (And used) on the North American Regional Reanalysis (NARR) as well as the ERA-5 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.ndimage as ndimage\n",
    "from datetime import date\n",
    "from itertools import repeat\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.path as mplPath\n",
    "from netCDF4 import Dataset\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "import os\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Utility Functions\n",
    "Run this code block to add required functions to the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_lat_lon(iReal, jReal, lon, lat):\n",
    "    '''\n",
    "    This interpolation routine functions by stripping the decimal value\n",
    "    from the i/j values and then interpolating between the two nearest \n",
    "    points along the lat/lon\n",
    "    '''\n",
    "    iDec = iReal % 1\n",
    "    jDec = jReal % 1\n",
    "    iInt = int(np.floor(iReal))\n",
    "    jInt = int(np.floor(jReal))\n",
    "\n",
    "    lonLow = lon[iInt, jInt]\n",
    "    lonHigh = lon[iInt + 1, jInt + 1]\n",
    "    latLow = lat[iInt, jInt]\n",
    "    latHigh = lat[iInt + 1, jInt + 1]\n",
    "\n",
    "    difLon = np.abs(lonHigh - lonLow)\n",
    "    difLat = np.abs(latHigh - latLow)\n",
    "\n",
    "    distLon = difLon * iDec\n",
    "    distLat = difLat * jDec\n",
    "\n",
    "    trueLon = lonLow + distLon\n",
    "    trueLat = latLow + distLat\n",
    "\n",
    "    return trueLon, trueLat\n",
    "\n",
    "def distance_matrix(lons,lats):\n",
    "    '''\n",
    "    Calculates the distances (in km) between any two cities based on the formulas\n",
    "    c = sin(lati1)*sin(lati2)+cos(longi1-longi2)*cos(lati1)*cos(lati2)\n",
    "    d = EARTH_RADIUS*Arccos(c)\n",
    "    where EARTH_RADIUS is in km and the angles are in radians.\n",
    "    Source: http://mathforum.org/library/drmath/view/54680.html\n",
    "    This function returns the matrix.\n",
    "    '''\n",
    "\n",
    "    EARTH_RADIUS = 6378.1\n",
    "    X = len(lons)\n",
    "    Y = len(lats)\n",
    "    assert X == Y, 'lons and lats must have same number of elements'\n",
    "\n",
    "    d = np.zeros((X,X))\n",
    "\n",
    "    #Populate the matrix.\n",
    "    for i2 in range(len(lons)):\n",
    "        lati2 = lats[i2]\n",
    "        loni2 = lons[i2]\n",
    "        c = np.sin(np.radians(lats)) * np.sin(np.radians(lati2)) + \\\n",
    "            np.cos(np.radians(lons-loni2)) * \\\n",
    "            np.cos(np.radians(lats)) * np.cos(np.radians(lati2))\n",
    "        d[c<1,i2] = EARTH_RADIUS * np.arccos(c[c<1])\n",
    "\n",
    "    return d\n",
    "\n",
    "def detect_storms(field, lon, lat, res, Npix_min, cyc):\n",
    "    '''\n",
    "    Detect storms present in field which satisfy the criteria.\n",
    "    Algorithm is an adaptation of an eddy detection algorithm,\n",
    "    outlined in Chelton et al., Prog. ocean., 2011, App. B.2,\n",
    "    with modifications needed for storm detection.\n",
    "\n",
    "    field is a 2D array specified on grid defined by lat and lon.\n",
    "\n",
    "    res is the horizontal grid resolution in degrees of field\n",
    "\n",
    "    Npix_min is the minimum number of pixels within which an\n",
    "    extremum of field must lie (recommended: 9).\n",
    "\n",
    "    cyc = 'cyclonic' or 'anticyclonic' specifies type of system\n",
    "    to be detected (cyclonic storm or high-pressure systems)\n",
    "\n",
    "    Function outputs lon, lat coordinates of detected storms\n",
    "    '''\n",
    "    len_deg_lat = 111.325 # length of 1 degree of latitude [km]\n",
    "\n",
    "    lon_storms = np.array([])\n",
    "    lat_storms = np.array([])\n",
    "    amp_storms = np.array([])\n",
    "\n",
    "    # Strip out the missing value flag\n",
    "    field[field==-9.969209968386869e+36] = np.nan\n",
    "\n",
    "    # ssh_crits is an array of ssh levels over which to perform storm detection loop\n",
    "    # ssh_crits increasing for 'cyclonic', decreasing for 'anticyclonic'\n",
    "    ssh_crits = np.linspace(np.nanmin(field), np.nanmax(field), 200)\n",
    "    ssh_crits.sort()\n",
    "    if cyc == 'anticyclonic':\n",
    "        ssh_crits = np.flipud(ssh_crits)\n",
    "\n",
    "    # loop over ssh_crits and remove interior pixels of detected storms from subsequent loop steps\n",
    "    for ssh_crit in ssh_crits:\n",
    "\n",
    "        # 1. Find all regions with eta greater (less than) than ssh_crit for anticyclonic (cyclonic) storms (Chelton et al. 2011, App. B.2, criterion 1)\n",
    "        if cyc == 'anticyclonic':\n",
    "            regions, nregions = ndimage.label( (field>ssh_crit).astype(int) )\n",
    "        elif cyc == 'cyclonic':\n",
    "            regions, nregions = ndimage.label( (field<ssh_crit).astype(int) )\n",
    "\n",
    "        for iregion in range(nregions): \n",
    "    # 2. Calculate number of pixels comprising detected region, reject if not within >= Npix_min\n",
    "            region = (regions==iregion+1).astype(int)\n",
    "            region_Npix = region.sum()\n",
    "            storm_area_within_limits = (region_Npix >= Npix_min)\n",
    "\n",
    "    # 3. Detect presence of local maximum (minimum) for anticylones (cyclones), reject if non-existent\n",
    "            interior = ndimage.binary_erosion(region)\n",
    "            exterior = region.astype(bool) ^ interior\n",
    "            if interior.sum() == 0:\n",
    "                continue\n",
    "            if cyc == 'anticyclonic':\n",
    "                has_internal_ext = field[interior].max() > field[exterior].max()\n",
    "            elif cyc == 'cyclonic':\n",
    "                has_internal_ext = field[interior].min() < field[exterior].min()\n",
    "\n",
    "    # 4. Find amplitude of region, reject if < amp_thresh\n",
    "            if cyc == 'anticyclonic':\n",
    "                amp_abs = field[interior].max()\n",
    "                amp = amp_abs - field[exterior].mean()\n",
    "            elif cyc == 'cyclonic':\n",
    "                amp_abs = field[interior].min()\n",
    "                amp = field[exterior].mean() - amp_abs\n",
    "            amp_thresh = np.abs(np.diff(ssh_crits)[0])\n",
    "            is_tall_storm = amp >= amp_thresh\n",
    "\n",
    "    # Quit loop if these are not satisfied\n",
    "            if np.logical_not(storm_area_within_limits * has_internal_ext * is_tall_storm):\n",
    "                continue\n",
    "\n",
    "    # Detected storms:\n",
    "            if storm_area_within_limits * has_internal_ext * is_tall_storm:\n",
    "                # find centre of mass of storm\n",
    "                storm_object_with_mass = field * region\n",
    "                storm_object_with_mass[np.isnan(storm_object_with_mass)] = 0\n",
    "                j_cen, i_cen = ndimage.center_of_mass(storm_object_with_mass)\n",
    "                lon_cen, lat_cen = interp_lat_lon(j_cen, i_cen, lon, lat)\n",
    "                # Save storm\n",
    "                lon_storms = np.append(lon_storms, lon_cen)\n",
    "                lat_storms = np.append(lat_storms, lat_cen)\n",
    "                # assign (and calculated) amplitude, area, and scale of storms\n",
    "                amp_storms = np.append(amp_storms, amp_abs)\n",
    "                # remove its interior pixels from further storm detection\n",
    "                storm_mask = np.ones(field.shape)\n",
    "                storm_mask[interior.astype(int)==1] = np.nan\n",
    "                field = field * storm_mask\n",
    "\n",
    "    return lon_storms, lat_storms, amp_storms\n",
    "\n",
    "def storms_list(lon_storms_a, lat_storms_a, amp_storms_a, lon_storms_c, lat_storms_c, amp_storms_c):\n",
    "    '''\n",
    "    Creates list detected storms\n",
    "    '''\n",
    "\n",
    "    storms = []\n",
    "\n",
    "    for ed in range(len(lon_storms_c)):\n",
    "        storm_tmp = {}\n",
    "        storm_tmp['lon'] = np.append(lon_storms_a[ed], lon_storms_c[ed])\n",
    "        storm_tmp['lat'] = np.append(lat_storms_a[ed], lat_storms_c[ed])\n",
    "        storm_tmp['amp'] = np.append(amp_storms_a[ed], amp_storms_c[ed])\n",
    "        storm_tmp['type'] = list(repeat('anticyclonic',len(lon_storms_a[ed]))) + list(repeat('cyclonic',len(lon_storms_c[ed])))\n",
    "        storm_tmp['N'] = len(storm_tmp['lon'])\n",
    "        storms.append(storm_tmp)\n",
    "\n",
    "    return storms\n",
    "\n",
    "def storms_init(det_storms, year, month, day, hour):\n",
    "    '''\n",
    "    Initializes list of storms. The ith element of output is\n",
    "    a dictionary of the ith storm containing information about\n",
    "    position and size as a function of time, as well as type.\n",
    "    '''\n",
    "\n",
    "    storms = []\n",
    "\n",
    "    for ed in range(det_storms[0][0]['N']):\n",
    "        storm_tmp = {}\n",
    "        storm_tmp['lon'] = np.array([det_storms[0][0]['lon'][ed]])\n",
    "        storm_tmp['lat'] = np.array([det_storms[0][0]['lat'][ed]])\n",
    "        storm_tmp['amp'] = np.array([det_storms[0][0]['amp'][ed]])\n",
    "        storm_tmp['type'] = det_storms[0][0]['type'][ed]\n",
    "        storm_tmp['year'] = np.array([year[0]])\n",
    "        storm_tmp['month'] = np.array([month[0]])\n",
    "        storm_tmp['day'] = np.array([day[0]])\n",
    "        storm_tmp['hour'] = np.array([hour[0]])\n",
    "        storm_tmp['exist_at_start'] = True\n",
    "        storm_tmp['terminated'] = False\n",
    "        storms.append(storm_tmp)\n",
    "\n",
    "    return storms\n",
    "\n",
    "def len_deg_lon(lat):\n",
    "    '''\n",
    "    Returns the length of one degree of longitude (at latitude\n",
    "    specified) in km.\n",
    "    '''\n",
    "\n",
    "    R = 6371. # Radius of Earth [km]\n",
    "\n",
    "    return (np.pi/180.) * R * np.cos( lat * np.pi/180. )\n",
    "\n",
    "\n",
    "def len_deg_lat():\n",
    "    '''\n",
    "    Returns the length of one degree of latitude in km.\n",
    "    '''\n",
    "    return 111.325 # length of 1 degree of latitude [km]\n",
    "\n",
    "\n",
    "def latlon2km(lon1, lat1, lon2, lat2):\n",
    "    '''\n",
    "    Returns the distance, in km, between (lon1, lat1) and (lon2, lat2)\n",
    "    '''\n",
    "\n",
    "    EARTH_RADIUS = 6371. # Radius of Earth [km]\n",
    "    c = np.sin(np.radians(lat1)) * np.sin(np.radians(lat2)) + np.cos(np.radians(lon1-lon2)) * np.cos(np.radians(lat1)) * np.cos(np.radians(lat2))\n",
    "    d = EARTH_RADIUS * np.arccos(c)\n",
    "\n",
    "    return d\n",
    "\n",
    "# Robert Note: Manipulating the prop_speed argument will control the search radius\n",
    "def track_storms(storms, det_storms, tt, year, month, day, hour, dt, prop_speed=140.):\n",
    "    '''\n",
    "    Given a set of detected storms as a function of time (det_storms)\n",
    "    this function will update tracks of individual storms at time step\n",
    "    tt in variable storms\n",
    "\n",
    "    dt indicates the time step of the underlying data (in hours)\n",
    "\n",
    "    prop_speed indicates the maximum storm propagation speed (in km/hour)\n",
    "    '''\n",
    "\n",
    "    # List of unassigned storms at time tt\n",
    "    unassigned = list(range(det_storms[tt][0]['N']))\n",
    "\n",
    "    # For each existing storm (t<tt) loop through unassigned storms and assign to existing storm if appropriate\n",
    "\n",
    "    for ed in range(len(storms)):\n",
    "\n",
    "        # Check if storm has already been terminated\n",
    "\n",
    "        if not storms[ed]['terminated']:\n",
    "\n",
    "            # Define search region around centroid of existing storm ed at last known position\n",
    "\n",
    "            x0 = storms[ed]['lon'][-1] # [deg. lon]\n",
    "            y0 = storms[ed]['lat'][-1] # [deg. lat]\n",
    "\n",
    "            # Find all storm centroids in search region at time tt\n",
    "\n",
    "            is_near = latlon2km(x0, y0, det_storms[tt][0]['lon'][unassigned], det_storms[tt][0]['lat'][unassigned]) <= prop_speed*dt\n",
    "\n",
    "            # Check if storms' type is the same as original storm\n",
    "\n",
    "            is_same_type = np.array([det_storms[tt][0]['type'][i] == storms[ed]['type'] for i in unassigned])\n",
    "\n",
    "            # Possible storms are those which are near and of the same type\n",
    "\n",
    "            possibles = is_near * is_same_type\n",
    "            if possibles.sum() > 0:\n",
    "\n",
    "                # Of all found storms, accept only the nearest one\n",
    "\n",
    "                dist = latlon2km(x0, y0, det_storms[tt][0]['lon'][unassigned], det_storms[tt][0]['lat'][unassigned])\n",
    "                nearest = dist == dist[possibles].min()\n",
    "                next_storm = unassigned[np.where(nearest * possibles)[0][0]]\n",
    "\n",
    "                # Add coordinatse and properties of accepted storm to trajectory of storm ed\n",
    "\n",
    "                storms[ed]['lon'] = np.append(storms[ed]['lon'], det_storms[tt][0]['lon'][next_storm])\n",
    "                storms[ed]['lat'] = np.append(storms[ed]['lat'], det_storms[tt][0]['lat'][next_storm])\n",
    "                storms[ed]['amp'] = np.append(storms[ed]['amp'], det_storms[tt][0]['amp'][next_storm])\n",
    "                storms[ed]['year'] = np.append(storms[ed]['year'], year[tt])\n",
    "                storms[ed]['month'] = np.append(storms[ed]['month'], month[tt])\n",
    "                storms[ed]['day'] = np.append(storms[ed]['day'], day[tt])\n",
    "                storms[ed]['hour'] = np.append(storms[ed]['hour'], hour[tt])\n",
    "\n",
    "                # Remove detected storm from list of storms available for assigment to existing trajectories\n",
    "\n",
    "                unassigned.remove(next_storm)\n",
    "\n",
    "                # Terminate storm otherwise\n",
    "\n",
    "            else:\n",
    "\n",
    "                storms[ed]['terminated'] = True\n",
    "\n",
    "    # Create \"new storms\" from list of storms not assigned to existing trajectories\n",
    "\n",
    "    if len(unassigned) > 0:\n",
    "\n",
    "        for un in unassigned:\n",
    "\n",
    "            storm_tmp = {}\n",
    "            storm_tmp['lon'] = np.array([det_storms[tt][0]['lon'][un]])\n",
    "            storm_tmp['lat'] = np.array([det_storms[tt][0]['lat'][un]])\n",
    "            storm_tmp['amp'] = np.array([det_storms[tt][0]['amp'][un]])\n",
    "            storm_tmp['type'] = det_storms[tt][0]['type'][un]\n",
    "            storm_tmp['year'] = year[tt]\n",
    "            storm_tmp['month'] = month[tt]\n",
    "            storm_tmp['day'] = day[tt]\n",
    "            storm_tmp['hour'] = hour[tt]\n",
    "            storm_tmp['exist_at_start'] = False\n",
    "            storm_tmp['terminated'] = False\n",
    "            storms.append(storm_tmp)\n",
    "\n",
    "    return storms\n",
    "\n",
    "\n",
    "def strip_storms(tracked_storms, dt, d_tot_min=1000., d_ratio=0.6, dur_min=72):\n",
    "    '''\n",
    "    Following Klotzbach et al. (MWR, 2016) strip out storms with:\n",
    "     1. A duration of less than dur_min (in hours). dt provides the\n",
    "        time step of the track data (in hours).\n",
    "     2. A total track length <= d_tot_min (short tracks)\n",
    "     3. A start-to-end straight-line distance that is less than d_ratio\n",
    "        times the total track length (meandering tracks).\n",
    "\n",
    "    Use d_tot_min = 0, d_ratio = 0 and/or dur_min = 0 to avoid stripping out\n",
    "    storms due to these criteria. It is recommended to use dur_min >= 6 or 12\n",
    "    hours in order to remove a significant number of \"storms\" that appear due\n",
    "    to high-frequency synoptic variability in the data.\n",
    "    '''\n",
    "\n",
    "    stripped_storms = []\n",
    "\n",
    "    for ed in range(len(tracked_storms)):\n",
    "\n",
    "        # 1. Remove storms which last less than dur_min hours\n",
    "        if len(tracked_storms[ed]['lon']) <= dur_min/dt:\n",
    "            continue\n",
    "\n",
    "        # 2. Calculate total track length\n",
    "        d_tot = 0\n",
    "        for k in range(len(tracked_storms[ed]['lon'])-1):\n",
    "            d_tot += latlon2km(tracked_storms[ed]['lon'][k], tracked_storms[ed]['lat'][k], tracked_storms[ed]['lon'][k+1], tracked_storms[ed]['lat'][k+1])\n",
    "\n",
    "        # 3. Calcualate start-to-end straight-line track distance\n",
    "        d_str = latlon2km(tracked_storms[ed]['lon'][0], tracked_storms[ed]['lat'][0], tracked_storms[ed]['lon'][-1], tracked_storms[ed]['lat'][-1])\n",
    "\n",
    "        # Keep storms that satisfy the conditions 2 & 3\n",
    "        if (d_tot >= d_tot_min) * ((d_str / d_tot) >= d_ratio):\n",
    "            stripped_storms.append(tracked_storms[ed])\n",
    "\n",
    "    return stripped_storms\n",
    "\n",
    "def classify_storms(tracked_storms):\n",
    "    '''\n",
    "    Robert: This function classifies storms based on the condition that they form inside a bounding area\n",
    "     and terminate outside of the formation area. At this moment we have supporting literature for\n",
    "     Alberta Clippers and Colorado Lows. Remaining cyclones are classified as \"other\"\n",
    "    '''\n",
    "    clipper_zone = [[-115,50],\n",
    "                    [-105,50],\n",
    "                    [-105,55],\n",
    "                    [-110,55],\n",
    "                    [-110,60],\n",
    "                    [-125,60],\n",
    "                    [-125,55],\n",
    "                    [-115,55]]\n",
    "    northwst_zone = [[-125, 60],\n",
    "                    [-115,60],\n",
    "                    [-115,65],\n",
    "                    [-125,65]]\n",
    "    colorado_zone = [[-105, 40],\n",
    "                    [-100,40],\n",
    "                    [-100,35],\n",
    "                    [-105,35]]\n",
    "    basin_zone =    [[-120, 45],\n",
    "                    [-115,45],\n",
    "                    [-115,35],\n",
    "                    [-120,35]]\n",
    "    gom_zone =      [[-100, 25],\n",
    "                    [-90,25],\n",
    "                    [-90,30],\n",
    "                    [-100,30]]\n",
    "    eastcst_zone =  [[-80, 30],\n",
    "                    [-75,30],\n",
    "                    [-75,35],\n",
    "                    [-65,35],\n",
    "                    [-65,45],\n",
    "                    [-70,45],\n",
    "                    [-70,40],\n",
    "                    [-80,40]]\n",
    "\n",
    "    clipperPoly = mplPath.Path(clipper_zone)\n",
    "    northwestPoly = mplPath.Path(northwst_zone)\n",
    "    coloradoPoly = mplPath.Path(colorado_zone)\n",
    "    basinPoly = mplPath.Path(basin_zone)\n",
    "    gomPoly = mplPath.Path(gom_zone)\n",
    "    eastCoastPoly = mplPath.Path(eastcst_zone)\n",
    "\n",
    "    classified_storms = []\n",
    "\n",
    "    for ed in range(len(tracked_storms)):\n",
    "        start_lon = tracked_storms[ed]['lon'][0]\n",
    "        start_lat = tracked_storms[ed]['lat'][0]\n",
    "        end_lon = tracked_storms[ed]['lon'][-1]\n",
    "        end_lat = tracked_storms[ed]['lat'][-1]\n",
    "\n",
    "        startPoint = (start_lon, start_lat)\n",
    "        endPoint = (end_lon, end_lat)\n",
    "        if clipperPoly.contains_point(startPoint, radius=1e-9) and not clipperPoly.contains_point(endPoint, radius=1e-9):\n",
    "            tracked_storms[ed]['classification'] = \"Clipper\"\n",
    "        elif northwestPoly.contains_point(startPoint, radius=1e-9) and not northwestPoly.contains_point(endPoint, radius=1e-9):\n",
    "            tracked_storms[ed]['classification'] = \"Northwest\"\n",
    "        elif coloradoPoly.contains_point(startPoint, radius=1e-9) and not coloradoPoly.contains_point(endPoint, radius=1e-9):\n",
    "            tracked_storms[ed]['classification'] = \"Colorado\"\n",
    "        elif basinPoly.contains_point(startPoint, radius=1e-9) and not basinPoly.contains_point(endPoint, radius=1e-9):\n",
    "            tracked_storms[ed]['classification'] = \"GreatBasin\"\n",
    "        elif gomPoly.contains_point(startPoint, radius=1e-9) and not gomPoly.contains_point(endPoint, radius=1e-9):\n",
    "            tracked_storms[ed]['classification'] = \"GulfOfMexico\"\n",
    "        elif eastCoastPoly.contains_point(startPoint, radius=1e-9) and not eastCoastPoly.contains_point(endPoint, radius=1e-9):\n",
    "            tracked_storms[ed]['classification'] = \"EastCoast\"\n",
    "        else:\n",
    "            tracked_storms[ed]['classification'] = \"Other\"\n",
    "\n",
    "        classified_storms.append(tracked_storms[ed])\n",
    "\n",
    "    return classified_storms\n",
    "\n",
    "# Robert: Added function to calculate Bergeron value for cyclones\n",
    "def calculate_bergeron(stormObject):\n",
    "    bergeronList = []\n",
    "    for i in range(len(stormObject['amp'])):\n",
    "        pressures = stormObject['amp'][i:i+8]\n",
    "        latitudes = stormObject['lat'][i:i+8]\n",
    "\n",
    "        angFactor = np.sin(np.radians(60)) / np.sin(np.radians(latitudes))\n",
    "\n",
    "        diffs = np.diff(pressures) / 100 #Convert Pa to mb\n",
    "        negatives = np.sum(val for val in diffs if val < 0)\t\n",
    "\n",
    "        bFactor = (-1 * np.sum(negatives)) / (24)\n",
    "\n",
    "        final = bFactor * angFactor\n",
    "\n",
    "        bergeronList.append(bFactor)\n",
    "    return bergeronList\n",
    "\n",
    "def timevector(date_start, date_end):\n",
    "    '''\n",
    "    Generated daily time vector, along with year, month, day, day-of-year,\n",
    "    and full date information, given start and and date. Format is a 3-element\n",
    "    list so that a start date of 3 May 2005 is specified date_start = [2005,5,3]\n",
    "    Note that day-of year (doy) is [0 to 59, 61 to 366] for non-leap years and [0 to 366]\n",
    "    for leap years.\n",
    "    returns: t, dates, T, year, month, day, doy\n",
    "    '''\n",
    "    # Time vector\n",
    "    t = np.arange(date(date_start[0],date_start[1],date_start[2]).toordinal(),date(date_end[0],date_end[1],date_end[2]).toordinal()+1)\n",
    "    T = len(t)\n",
    "    # Date list\n",
    "    dates = [date.fromordinal(tt.astype(int)) for tt in t]\n",
    "    # Vectors for year, month, day-of-month\n",
    "    year = np.zeros((T))\n",
    "    month = np.zeros((T))\n",
    "    day = np.zeros((T))\n",
    "    for tt in range(T):\n",
    "        year[tt] = date.fromordinal(t[tt]).year\n",
    "        month[tt] = date.fromordinal(t[tt]).month\n",
    "        day[tt] = date.fromordinal(t[tt]).day\n",
    "    year = year.astype(int)\n",
    "    month = month.astype(int)\n",
    "    day = day.astype(int)\n",
    "    # Leap-year baseline for defining day-of-year values\n",
    "    year_leapYear = 2012 # This year was a leap-year and therefore doy in range of 1 to 366\n",
    "    t_leapYear = np.arange(date(year_leapYear, 1, 1).toordinal(),date(year_leapYear, 12, 31).toordinal()+1)\n",
    "    dates_leapYear = [date.fromordinal(tt.astype(int)) for tt in t_leapYear]\n",
    "    month_leapYear = np.zeros((len(t_leapYear)))\n",
    "    day_leapYear = np.zeros((len(t_leapYear)))\n",
    "    doy_leapYear = np.zeros((len(t_leapYear)))\n",
    "    for tt in range(len(t_leapYear)):\n",
    "        month_leapYear[tt] = date.fromordinal(t_leapYear[tt]).month\n",
    "        day_leapYear[tt] = date.fromordinal(t_leapYear[tt]).day\n",
    "        doy_leapYear[tt] = t_leapYear[tt] - date(date.fromordinal(t_leapYear[tt]).year,1,1).toordinal() + 1\n",
    "    # Calculate day-of-year values\n",
    "    doy = np.zeros((T))\n",
    "    for tt in range(T):\n",
    "        doy[tt] = doy_leapYear[(month_leapYear == month[tt]) * (day_leapYear == day[tt])]\n",
    "    doy = doy.astype(int)\n",
    "\n",
    "    return t, dates, T, year, month, day, doy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storm Detection\n",
    "The first step of the analysis is to run the storm detection block. This code loops through the incoming data and runs the eddy detection code on it, a giant list object containing each time step and properties of each eddy at each time step is written out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a multiprocessing-ready function that is used to delegate tasks\n",
    "across the multiprocessing.map function to allow for quicker processing.\n",
    "'''\n",
    "def run_detection(slp, idx, size, lon, lat):\n",
    "    processStart = time.time()\n",
    "    lon_storms_a = []\n",
    "    lat_storms_a = []\n",
    "    amp_storms_a = []\n",
    "    lon_storms_c = []\n",
    "    lat_storms_c = []\n",
    "    amp_storms_c = []\n",
    "    # anti-cyclones\n",
    "    lon_storms, lat_storms, amp = detect_storms(slp, lon, lat, res=2, Npix_min=9, cyc='anticyclonic')\n",
    "    lon_storms_a.append(lon_storms)\n",
    "    lat_storms_a.append(lat_storms)\n",
    "    amp_storms_a.append(amp)\n",
    "    # cyclones\n",
    "    lon_storms, lat_storms, amp = detect_storms(slp, lon, lat, res=2, Npix_min=9, cyc='cyclonic')\n",
    "    lon_storms_c.append(lon_storms)\n",
    "    lat_storms_c.append(lat_storms)\n",
    "    amp_storms_c.append(amp)\n",
    "    # Write out\n",
    "    storms = storms_list(lon_storms_a, lat_storms_a, amp_storms_a, lon_storms_c, lat_storms_c, amp_storms_c)\n",
    "    processEnd = time.time()\n",
    "    print(\"Time step completed (\" + str(idx) + \" / \" + str(size) + \"), Elapsed Time: \" + time.strftime(\"%H:%M:%S\", time.gmtime(processEnd - processStart)))\n",
    "    return {idx: storms}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block of code runs the detection routine. \n",
    "\n",
    "**NOTE:** This will take a lot of time depending on how many files you are trying to process, how big the files are, and how many processors you have on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load in slp data and lat/lon coordinates\n",
    "#\n",
    "print(\"Program Start...\")\n",
    "\n",
    "# Parameters\n",
    "## NOTE: MAKE SURE YOU EDIT THIS LINE!!!!\n",
    "## THIS IS WHERE THE PROGRAM WILL LOOK FOR DATA\n",
    "dataDir = ''\n",
    "# This is the variable that is pulled from the netCDF file\n",
    "var = 'mslet'\n",
    "\n",
    "# Generate date and hour vectors\n",
    "yearStart = 1979\n",
    "yearEnd = 2020 \n",
    "\n",
    "# Load lat, lon\n",
    "filename = dataDir + \"mslet.\" + str(yearStart) + \".nc\"\n",
    "print(\"Loading in first netCDF file to populate arrays... \" + str(filename[dataset]))\n",
    "fileobj = Dataset(filename[dataset], 'r')\n",
    "lon = fileobj.variables['lon'][:].astype(float)\n",
    "lat = fileobj.variables['lat'][:].astype(float)\n",
    "fileobj.close()\n",
    "\n",
    "bigListStorms = []\n",
    "\n",
    "print(\"Spawning a multiprocessing pool, \" + str(os.cpu_count()) + \" processors detected.\")\n",
    "pool = multiprocessing.Pool(os.cpu_count())\n",
    "\n",
    "print(\"Entering loop, beginning timer.\")\n",
    "fullStart = time.time()\n",
    "yListTime = []\n",
    "\n",
    "# Create empty arrays to hold the data\n",
    "year = np.zeros((0,))\n",
    "month = np.zeros((0,))\n",
    "day = np.zeros((0,))\n",
    "hour = np.zeros((0,))\n",
    "\n",
    "for yr in range(yearStart, yearEnd+1):\n",
    "    save_storms = {}\n",
    "    current_list_storms = []\n",
    "\n",
    "    innerTimeStart = time.time()\n",
    "\n",
    "    filename = dataDir + \"mslet.\" + str(yr) + \".nc\"\n",
    "    fileobj = Dataset(filename[dataset], 'r')\n",
    "    timeAr = fileobj.variables['time'][:]\n",
    "    time_ordinalDays = timeAr/24. + date(1800,1,1).toordinal()\n",
    "    year = np.append(year, [date.fromordinal(np.floor(time_ordinalDays[tt]).astype(int)).year for tt in range(len(timeAr))])\n",
    "    month = np.append(month, [date.fromordinal(np.floor(time_ordinalDays[tt]).astype(int)).month for tt in range(len(timeAr))])\n",
    "    day = np.append(day, [date.fromordinal(np.floor(time_ordinalDays[tt]).astype(int)).day for tt in range(len(timeAr))])\n",
    "    hour = np.append(hour, (np.mod(time_ordinalDays, 1)*24).astype(int))\n",
    "    slp0 = fileobj.variables[var[dataset]][:].astype(float)\n",
    "    fileobj.close()\n",
    "\n",
    "    # Run the multiprocessed detection function, merge the returned dictionary object with our local one.\n",
    "    out_dict = pool.starmap(run_detection, zip(slp0, np.arange(slp0.shape[0]), itertools.repeat(slp0.shape[0]), itertools.repeat(lon), itertools.repeat(lat)))\n",
    "    ro_inner = time.time()\n",
    "    for iDict in out_dict: #Note: starmap returns as a 0-length list ie [return] with the Nth element being each input\n",
    "        save_storms = {**save_storms, **iDict} \n",
    "    # Order the final list correctly\n",
    "    for i in range(slp0.shape[0]):\n",
    "        current_list_storms.append(save_storms[i])\n",
    "    ro_outer = time.time() - ro_inner\n",
    "    print(str(yr) + \" List Reordering Completed, Elapsed Time: \" + time.strftime(\"%H:%M:%S\", time.gmtime(ro_outer)))\n",
    "    # Append to the big list, then save.\n",
    "    bigListStorms.append(current_list_storms)\n",
    "    innerTimeEnd = time.time()\n",
    "    yListTime.append((innerTimeEnd - innerTimeStart))\n",
    "    print(str(yr) + \" Completed, Elapsed Time: \" + time.strftime(\"%H:%M:%S\", time.gmtime(innerTimeEnd - innerTimeStart)))\n",
    "\n",
    "    np.savez('storm_det_slp', storms=bigListStorms, year=year, month=month, day=day, hour=hour)\n",
    "\n",
    "pool.close() \n",
    "pool.join()\n",
    "\n",
    "fullEnd = time.time()\n",
    "print(\"Program Completed, Elapsed Time: \" + time.strftime(\"%H:%M:%S\", time.gmtime(fullEnd - fullStart)))\n",
    "\n",
    "for i, yi in enumerate(yListTime):\n",
    "    print(\"Processing Time (\" + str(yearStart + i) + \"): \" + time.strftime(\"%H:%M:%S\", time.gmtime(yi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storm Tracking\n",
    "After completing the above storm detection code, you will have a *storm_det_slp.npz* file generated, as mentioned this contains a giant list of time steps from which all eddies are marked within.\n",
    "\n",
    "This code block runs the eddy tracking routine, and applies the tracking restrictions as defined by Klotzbach et al. (MWR, 2016) to remove any eddies that do not behave like storms. Additional criterion may be considered and can be added in at the specified point.\n",
    "\n",
    "No multiprocessing was introduced for this routine, so it will take some time to run through completely, although for most cases, I noted that ~40 years of data completes in about 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Program Start...\")\n",
    "\n",
    "# Pass the storm_det_slp file generated in the previous block to this\n",
    "# call here.\n",
    "data = np.load('storm_det_slp.npz', encoding='latin1', allow_pickle=True)\n",
    "\n",
    "det_storms = data['storms']\n",
    "year = data['year']\n",
    "month = data['month']\n",
    "day = data['day']\n",
    "hour = data['hour']\n",
    "\n",
    "# Initialize storms discovered at first time step\n",
    "\n",
    "storms = storms_init(det_storms, year, month, day, hour)\n",
    "\n",
    "# Stitch storm tracks together at future time steps\n",
    "T = len(det_storms) # number of time steps\n",
    "print(\"Preparing to enter loop, there are \" + str(T) + \" total time steps to evaluate\")\n",
    "processStart = time.time()\n",
    "avgLoop = 0\n",
    "for tt in range(0, T):\n",
    "    # Track storms from time step tt-1 to tt and update corresponding tracks and/or create new storms\n",
    "    loopTimeStart = time.time()\n",
    "    storms = track_storms(storms, det_storms, tt, year, month, day, hour, dt=3)\n",
    "    loopTimeEnd = time.time()\n",
    "    procTime = loopTimeEnd - loopTimeStart\n",
    "    avgLoop += procTime\n",
    "\n",
    "    if(tt % 1000 == 0 and tt != 0):\n",
    "        print(\"Evaluation: \" + str(tt) + \"/\" + str(T) + \" - Avg. Time: \" + \n",
    "        time.strftime(\"%H:%M:%S\", time.gmtime(avgLoop / tt))\n",
    "        + \", Est. Completion: \" + time.strftime(\"%H:%M:%S\", time.gmtime(avgLoop * (T - tt))))\n",
    "processEnd = time.time()\n",
    "print(\"Loop completed, total time: \" + time.strftime(\"%H:%M:%S\", time.gmtime(processEnd - processStart)))\n",
    "\n",
    "# Add keys for storm age and flag if storm was still in existence at end of run\n",
    "for ed in range(len(storms)):\n",
    "    storms[ed]['age'] = len(storms[ed]['lon'])\n",
    "\n",
    "# Robert: Added Classification here\n",
    "print(\"Classifying Storms...\")\n",
    "storms = classify_storms(storms)\n",
    "\n",
    "# Strip storms based on track lengths (dt = 3hr, d_tot_min = 500km, dur_min = 24hr)\n",
    "print(\"Removing Storms that do not meet criteron...\")\n",
    "storms = strip_storms(storms, dt=3, d_tot_min=500., d_ratio=0.6, dur_min=24)\n",
    "\n",
    "# Save tracked storm data\n",
    "print(\"Saving output file...\")\n",
    "np.savez('storm_track_slp', storms=storms)\n",
    "\n",
    "print(\"Program Complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
